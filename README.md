<p style="width: 100%">
<img src="https://github.com/we-like-parsers/pegen/raw/main/media/logo.svg" style="display: block; margin: 0 auto; width: 70%">
</p>

-----------------------------------

[![Downloads](https://pepy.tech/badge/pegen/month)](https://pepy.tech/project/pegen)
[![PyPI version](https://badge.fury.io/py/pegen.svg)](https://badge.fury.io/py/pegen)
![CI](https://github.com/we-like-parsers/pegen/actions/workflows/test.yml/badge.svg)

# What is this?

Pegen is the parser generator used in CPython to produce the parser used by the interpreter. It allows to
produce PEG parsers from a description of a formal Grammar.

## About this fork
Pegen's logic is tied to the way Python is parsed. This fork tries to decouple
Pegen's logic with Python language's.

# Installing

Install with `pip` or your favorite PyPI package manager.

```
pip install pegen
```

# Documentation

The documentation is available [here](https://we-like-parsers.github.io/pegen/).

# How to generate a parser

Given a grammar file compatible with `pegen` (you can write your own or start with one in the [`data`](data) directory), you
can easily generate a parser by running:

```
python -m pegen <path-to-grammar-file> -o parser.py
```

This will generate a file called `parser.py` in the current directory. This can be used to parse code using the grammar that
we just used:

```
python parser.py <file-with-code-to-parse>
```

As a demo: generate a Python parser from data/python.gram, and use the generated parser to parse and run tests/demo.py
```
make demo
```


# How to contribute

See the instructions in the [CONTRIBUTING.md](CONTRIBUTING.md) file.

# Differences with CPython's Pegen

This repository exists to distribute a version of the Python PEG parser generator used by CPython that can be installed via PyPI,
with some improvements. Although the official PEG generator included in CPython can generate both Python and C code, this distribution
of the generator only allows to generate Python code. This is due to the fact that the C code generated by the generator included
in CPython includes a lot of implementation details and private headers that are not available for general usage.

The official PEG generator for Python 3.9 and later is now included in the CPython repo under
[Tools/peg_generator/](https://github.com/python/cpython/tree/master/Tools/peg_generator). We aim to keep this repo in sync with the
Python generator from that version of `pegen`.

See also [PEP 617](https://www.python.org/dev/peps/pep-0617/).

# Repository structure

* The `src` directory contains the `pegen` source (the package itself).
* The `tests` directory contains the test suite for the `pegen` package.
* The `data` directory contains some example grammars compatible with `pegen`. This
  includes a [pure-Python version of the Python grammar](data/python.gram).
* The `docs` directory contains the documentation for the package.
* The `scripts` directory contains some useful scripts that can be used for visualizing
  grammars, benchmarking and other usages relevant to the development of the generator itself.
* The `stories` directory contains the backing files and examples for
  [Guido's series on PEG parser](https://medium.com/@gvanrossum_83706/peg-parsing-series-de5d41b2ed60).


# Quick syntax overview

The grammar consists of a sequence of rules of the form:

```
rule_name: expression
```

A type annotation can be included after the rule name, in square brackets,
which annotates the type of the parse result:

```
rule_name[ReturnType]: expression
```

In the absence of a type annotation, the generated code for a rule
is annotated to return `typing.Any`. This has no effect on its execution
but may upset type checkers and make the grammar a little harder to understand.

## Grammar Expressions

> **Note:** Below "token" refers to tokens produced by Python `tokenize` unless the context suggests otherwise.

### `# comment`

Python-style comments.

### `TOKENNAME`

The following all-uppercase names are special and matches a token of the same name:

<!--Inducted from PythonCallMakerVisitor.visit_NameLeaf-->

- [`SOFT_KEYWORD`][soft_keyword]: [XXX: ?]

  [XXX: [The tokenizer never produces this value in latest version][soft_keyword]?]
- [`NAME`][name]: [Python identifiers and soft keywords][identifiers]
  > Replaced by [token rule `identifier`][identifiers] in latest grammar.
  > [Last formal appearance in grammar spec][names_keywords-3.14]

  `NAME`s are prevented to collide with hard keywords in `'string'` expressions
  *before* matching syntax rules; see `'string'` below.
- [`NUMBER`][number]: A [Python numeric literal (int, float, complex)][numeric_literals]
  > Definition disappeared in latest grammar. The [rule `literal`][literal_rule],
  > which subsumes [original definition of `NUMBER`][names_keywords-3.14],
  > is used in most places; "Matching just a numeric literal" is not seen.
- [`STRING`][string_token]: A [string or bytes literal][strings] (not including f-string literals)
- [`FSTRING_START`][fstring_start]: Token used to indicate the beginning of an f-string literal
- [`FSTRING_MIDDLE`][fstring_middle]: Token used for literal text inside an f-string literal, including format specifications
- [`FSTRING_END`][fstring_end]: Token used to indicate the end of a f-string
- [`OP`][op]: A generic token value that indicates an [operator][operators] or [delimiter][delimiters]
- [`TYPE_COMMENT`][type_comment]
- [`NEWLINE`][newline]: Indicates the end of a [logical line][logical_lines]
- [`DEDENT`][dedent]: Indicates the end of an [indented block][indentation]
- [`INDENT`][indent]: Indicates the start of an [indented block][indentation]
- [`ENDMARKER`][endmarker]: Indicates the end of input
- [`ASYNC`][async-3.12], [`AWAIT`][await-3.12]

  Removed in Python 3.13.

[soft_keyword]: https://docs.python.org/3/library/token.html#token.SOFT_KEYWORD
[name]: https://docs.python.org/3/library/token.html#token.NAME
[identifiers]: https://docs.python.org/3/reference/lexical_analysis.html#identifiers
[names_keywords-3.14]: https://docs.python.org/3.14/reference/lexical_analysis.html#names-identifiers-and-keywords
[number]: https://docs.python.org/3/library/token.html#token.NUMBER
[numeric_literals]: https://docs.python.org/3/reference/lexical_analysis.html#numeric-literals
[literal_rule]: https://docs.python.org/3/reference/expressions.html#grammar-token-python-grammar-literal
[string_token]: https://docs.python.org/3/library/token.html#token.STRING
[strings]: https://docs.python.org/3/reference/lexical_analysis.html#strings
[fstring_start]: https://docs.python.org/3/library/token.html#token.FSTRING_START
[fstring_middle]: https://docs.python.org/3/library/token.html#token.FSTRING_MIDDLE
[fstring_end]: https://docs.python.org/3/library/token.html#token.FSTRING_END
[op]: https://docs.python.org/3/library/token.html#token.OP
[operators]: https://docs.python.org/3/reference/lexical_analysis.html#operators
[delimiters]: https://docs.python.org/3/reference/lexical_analysis.html#delimiters
[type_comment]: https://docs.python.org/3/library/token.html#token.TYPE_COMMENT
[newline]: https://docs.python.org/3/library/token.html#token.NEWLINE
[logical_lines]: https://docs.python.org/3/reference/lexical_analysis.html#logical-lines
[dedent]: https://docs.python.org/3/library/token.html#token.DEDENT
[indentation]: https://docs.python.org/3/reference/lexical_analysis.html#indentation
[indent]: https://docs.python.org/3/library/token.html#token.INDENT
[endmarker]: https://docs.python.org/3/library/token.html#token.ENDMARKER
[async-3.12]: https://docs.python.org/3.12/library/token.html#token.ASYNC
[await-3.12]: https://docs.python.org/3.12/library/token.html#token.AWAIT

> **Note:** Although still produced by `tokenize`,
> the notion of some tokens in the grammar seem to be reducing,
> as they are no longer formally defined in the grammar description
> of the latest version, such as
> - `NAME` replaced by [`identifier`](https://docs.python.org/3/reference/lexical_analysis.html#identifiers)
> - `NUMBER` replaced by [`integer | floatnumber | imagnumber` or just being used through `literal` in latest docs](https://docs.python.org/3/reference/expressions.html#literals).

### `'string'`

Takes a single token, and matches it against string contents.

Always fails when the string contents is not describable by a single token
(e.g. `'(a'`, `'$'`).

Consecutive `'string'` and `"string"`s are matched in tokenized style. For example,
`'a' '+' 'b'` matches `a+b`, `a  +  b`, and `'a' 'b'` doesn't match `ab`.

If string contents matches the regular expression `[a-zA-Z_]\w*\Z`,
it will be recorded as a hard keyword in the syntax.

The generated code prevents `NAME` tokens anywhere in the parsed text
to collide with hard keywords, *before* syntax rules are matched,
unless when the rule explicitly asks for a hard keyword that matches the token.
For example,

```
rule: "hello" NAME | 'world'
```

will not match

```
hello world
```

even though the branch `'world'` is not chosen; but matching

```
world
```

will succeed.

<!--This example is verified in test_pegen.py:test_hard_keywords-->

### `"string"`

Takes a single token, and matches it against string contents.

Always fails when the string contents is not describable by a single token
(e.g. `"(a"`, `"$"`).

Consecutive `'string'` and `"string"`s are matched in tokenized style. For example,
`"a" "+" "b"` matches `a+b`, `a  +  b`, and `"a" "b"` doesn't match `ab`.

If string contents matches the regular expression `[a-zA-Z_]\w*\Z`,
it will be recorded as a soft keyword in the syntax.

`NAME` tokens in the parsed text are allowed to collide with soft keywords.
<!--TODO: Wording is OK?-->

<!--At src/pegen/python_generator.py PythonCallMakerVisitor.visit_StringLeaf-->

### `e1 e2`

Match e1, then match e2.

```
rule_name: first_rule second_rule
```

### `e1 | e2`

Match e1 or e2.

```
rule_one: cool_rule | not_so_cool_rule

rule_two: cool_rule | not_so_cool_rule
    | rule_one
```

The first alternative can also appear on the line after the rule name
for formatting purposes. In that case, a \| must be used before the
first alternative, like so:

```
rule_name[ReturnType]:
    | first_alt
    | second_alt
```

### `( e )`

Grouping operator.

```
rule_name: (e1 | e2)*
```

### `[ e ] or e?`

Optionally match e.

```
rule_one: [e]
rule_two: e?
```

A more useful example includes defining that a trailing comma is
optional:

```
rule_name: e (',' e)* [',']
```

This can be more conveniently written with `s.e+`:

```
rule_name: ','.e+ [',']
```

### `e*`

Match zero or more occurrences of e.

```
rule_name: (e1 e2)*
```

### `e+`

Match one or more occurrences of e.

```
rule_name: (e1 e2)+
```

### `s.e+`

Match one or more occurrences of e, separated by s. The generated parse
tree does not include the separator. This is otherwise identical to
``(e (s e)*)``.

```
rule_name: ','.e+
```

### `&e`

Succeed if e can be parsed, without consuming any input.

### `&&e`

[XXX: What does this do?]

### `!e`

Fail if e can be parsed, without consuming any input.

An example taken from the Python grammar specifies that a primary
consists of an atom, which is not followed by a ``.`` or a ``(`` or a
``[``:

```
primary: atom !'.' !'(' !'['
```

### `~`

Commit to the current alternative, even if it fails to parse.

```
rule_name: '(' ~ some_rule ')' | some_alt
```

In this example, if a left parenthesis is parsed, then the other
alternative won’t be considered, even if some_rule or ‘)’ fail to be
parsed.

## Left recursion

PEG parsers normally do not support left recursion but Pegen implements a
technique that allows left recursion using the memoization cache. This allows
us to write not only simple left-recursive rules but also more complicated
rules that involve indirect left-recursion like

```
rule1: rule2 | 'a'
rule2: rule3 | 'b'
rule3: rule1 | 'c'
```

and "hidden left-recursion" like

```
rule: 'optional'? rule '@' some_other_rule
```

## Variables in the Grammar
[TODO: Collection rules?]
An expression can be captured by preceding it with an identifier and an
``=`` sign. The name can then be used in the action (see below), like this:

```
start[int]: '(' a=contents ')' {
    sum([1 if token.string == "a" else 0 for token in a]) }
contents: ("a" | "b")*
```

The above parser counts the number of "a"s in a pair of brackets:
for example, parsing `(a b a)` returns 2.

Rule uses and token uses that do not have a capture
will be automatically captured by its lowercased name:

```
rule_one[int]: NUMBER { int(number) * 2 }  # Captures NUMBER as number
rule_two[int]: rule_one { rule_one % 17 }  # Captures rule_one as rule_one
```

[TODO: Duplicate names?]

However, expressions in the form `a+`, `a*` etc. are
currently not automatically captured.

```
#XXX: Type of rule_three?
rule_three[int]: two=rule_two* { ... }  # "two=" required here
```

## Grammar actions

To avoid the intermediate steps that obscure the relationship between the
grammar and the AST generation the PEG parser allows directly generating AST
nodes for a rule via grammar actions. Grammar actions are language-specific
expressions that are evaluated when a grammar rule is successfully parsed. These
expressions can be written in Python. As an example of a grammar with Python actions,
the piece of the parser generator that parses grammar files is bootstrapped from a
meta-grammar file with Python actions that generate the grammar tree as a result
of the parsing.

In the specific case of the PEG grammar for Python, having actions allows
directly describing how the AST is composed in the grammar itself, making it
more clear and maintainable. This AST generation process is supported by the use
of some helper functions that factor out common AST object manipulations and
some other required operations that are not directly related to the grammar.

To indicate these actions each alternative can be followed by the action code
inside curly-braces, which specifies the return value of the alternative

```
rule_name[ReturnType]:
    | first_alt1 first_alt2 { first_alt1 }
    | second_alt1 second_alt2 { second_alt1 }
```

If the action is omitted, a default action is generated:

* If there's a single name in the rule in the rule, it gets returned.

* If there is more than one name in the rule, a collection with all parsed
  expressions gets returned.

This default behaviour is primarily made for very simple situations and for
debugging purposes.

As an illustrative example this simple grammar file generates a full parser
that can parse simple arithmetic expressions and return a valid Python AST:

```
start[ast.Module]: a=expr_stmt* ENDMARKER { ast.Module(body=a or []) }
expr_stmt: a=expr NEWLINE { ast.Expr(value=a, EXTRA) }

expr:
    | l=expr '+' r=term { ast.BinOp(left=l, op=ast.Add(), right=r, EXTRA) }
    | l=expr '-' r=term { ast.BinOp(left=l, op=ast.Sub(), right=r, EXTRA) }
    | term

term:
    | l=term '*' r=factor { ast.BinOp(left=l, op=ast.Mult(), right=r, EXTRA) }
    | l=term '/' r=factor { ast.BinOp(left=l, op=ast.Div(), right=r, EXTRA) }
    | factor

factor:
    | '(' e=expr ')' { e }
    | atom

atom: NUMBER
```

In comparison, this grammar file computes the expression
instead of generating an AST:

```
start[ast.Module]: a=expr_stmt* ENDMARKER { ast.Module(body=a or []) }
expr_stmt: a=expr NEWLINE { ast.Expr(value=a, EXTRA) }

expr:
    | l=expr '+' r=term { ast.BinOp(left=l, op=ast.Add(), right=r, EXTRA) }
    | l=expr '-' r=term { ast.BinOp(left=l, op=ast.Sub(), right=r, EXTRA) }
    | term

term:
    | l=term '*' r=factor { ast.BinOp(left=l, op=ast.Mult(), right=r, EXTRA) }
    | l=term '/' r=factor { ast.BinOp(left=l, op=ast.Div(), right=r, EXTRA) }
    | factor

factor:
    | '(' e=expr ')' { e }
    | atom

atom:
    | NUMBER { float(number) }
```

[TODO: Default header]

### Special names in actions
[TODO]
<!--src\pegen\python_generator.py:411-->
LOCATIONS
UNREACHABLE
EXTRA?

## How to use the generated parser
[TODO]

## Entry point
[TODO]
The `start` rule is the entry point of the generated parser,
but users can also call other rules.

## Metas
[TODO]